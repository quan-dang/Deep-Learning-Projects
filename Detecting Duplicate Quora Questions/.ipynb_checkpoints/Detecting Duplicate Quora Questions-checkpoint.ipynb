{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Duplicate Quora Questions\n",
    "## Dataset \n",
    "1. __Question Pairs Dataset from Quora__\n",
    "    \n",
    "    __Link:__ https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs\n",
    "\n",
    "    The dataset consists of over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line truly contains a duplicate pair.\n",
    "    \n",
    "    \n",
    "2. __GloVe Embeddings: Global Vectors for Word Representation__\n",
    "   \n",
    "   __Link:__ http://nlp.stanford.edu/data/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/quora_duplicate_questions.tsv', sep='\\t')\n",
    "data = data.drop(['id', 'qid1', 'qid2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length-based features\n",
    "data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
    "data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
    "\n",
    "# differences in lengths of two questions\n",
    "data['diff_len'] = data.len_q1 - data.len_q2\n",
    "\n",
    "# character length based features\n",
    "data['len_char_q1'] = data.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data['len_char_q2'] = data.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "\n",
    "# word length based features\n",
    "data['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\n",
    "data['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# common words in the two questions using intersection\n",
    "data['common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split())\n",
    "                                 .intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "\n",
    "# mark all the above set of features as feature set-1 (fs_1)\n",
    "fs_1 = ['len_q1', 'len_q2', 'diff_len', 'len_char_q1', \n",
    "        'len_char_q2', 'len_word_q1', 'len_word_q2',     \n",
    "        'common_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine fuzzywuzzy \n",
    "The next set of features are based on __fuzzy string matching.__\n",
    "\n",
    "__Package required:__ __fuzzywuzzy__ and __python-levenshtein__ (an important dependency of __fuzzywuzzy__ for faster processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine fuzz.QRatio (a higher Qration value means higher similarity bw two questions)\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "fuzz.QRatio(\"Why did Trump win the Presidency?\", \n",
    "           \"How did Donald Trump win the 2016 Presidential Election\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.QRatio(\"How can I start an online shopping (e-commerce) website?\", \n",
    "            \"Which web technology is best suitable for building a big E-Commerce website?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine fuzz.partial_ratio (a notably difference bw the above two cases when using partial_ratio)\n",
    "fuzz.partial_ratio(\"Why did Trump win the Presidency?\", \n",
    "   \"How did Donald Trump win the 2016 Presidential Election\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio(\"How can I start an online shopping (e-commerce) website?\", \n",
    "                   \"Which web technology is best suitable for building a big E-Commerce website?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create another set of features using fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(\n",
    "    str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "data['fuzz_WRatio'] = data.apply(lambda x: fuzz.WRatio(\n",
    "    str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "data['fuzz_partial_ratio'] = data.apply(lambda x: \n",
    "                    fuzz.partial_ratio(str(x['question1']), \n",
    "                    str(x['question2'])), axis=1)\n",
    "\n",
    "data['fuzz_partial_token_set_ratio'] = data.apply(lambda x: \n",
    "                    fuzz.partial_token_set_ratio(str(x['question1']), \n",
    "                    str(x['question2'])), axis=1)\n",
    "\n",
    "data['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: \n",
    "                    fuzz.partial_token_sort_ratio(str(x['question1']), \n",
    "                    str(x['question2'])), axis=1)\n",
    "\n",
    "data['fuzz_token_set_ratio'] = data.apply(lambda x: \n",
    "                    fuzz.token_set_ratio(str(x['question1']), \n",
    "                    str(x['question2'])), axis=1)\n",
    "\n",
    "data['fuzz_token_sort_ratio'] = data.apply(lambda x: \n",
    "                    fuzz.token_sort_ratio(str(x['question1']), \n",
    "                    str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark all the above set of features as feature set-2 (fs_2)\n",
    "fs_2 = ['fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio', \n",
    "       'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
    "       'fuzz_token_set_ratio', 'fuzz_token_sort_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and SVD features\n",
    "The 3rd set of features isa combination of TFIDF and SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for TfidfVectorizer have been proved to work well for a wide range of NLP, especially text classification\n",
    "# one might need to change is stop_words\n",
    "tfv_q1 = TfidfVectorizer(min_df=3,\n",
    "                        max_features=None,\n",
    "                        strip_accents='unicode',\n",
    "                        analyzer='word',\n",
    "                        token_pattern=r'\\w{1,}',\n",
    "                        ngram_range=(1, 2),\n",
    "                        use_idf=1,\n",
    "                        smooth_idf=1,\n",
    "                        sublinear_tf=1,\n",
    "                        stop_words='english')\n",
    "\n",
    "tfv_q2 = deepcopy(tfv_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_tfidf = tfv_q1.fit_transform(data.question1.fillna(\"\"))\n",
    "q2_tfidf = tfv_q2.fit_transform(data.question2.fillna(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd_q1 = TruncatedSVD(n_components=180)\n",
    "svd_q2 = TruncatedSVD(n_components=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1_vectors = svd_q1.fit_transform(q1_tfidf)\n",
    "question2_vectors = svd_q2.fit_transform(q2_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "data['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "\n",
    "data['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# stack two different TF-IDFs for the 2 questions horizontally and feed to a machine learning model\n",
    "# for text columns, sparse.hstack might be a good choice\n",
    "fs3_1 = sparse.hstack((q1_tfidf, q2_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine two questions and calculate ft-idf\n",
    "tfv = TfidfVectorizer(min_df=3, \n",
    "                      max_features=None, \n",
    "                      strip_accents='unicode', \n",
    "                      analyzer='word', \n",
    "                      token_pattern=r'\\w{1,}',\n",
    "                      ngram_range=(1, 2), \n",
    "                      use_idf=1, \n",
    "                      smooth_idf=1, \n",
    "                      sublinear_tf=1,\n",
    "                      stop_words='english')\n",
    "\n",
    "q1q2 = data.question1.fillna(\"\")\n",
    "q1q2 = q1q2 + \" \" + data.question2.fillna(\"\")\n",
    "fs3_2 = tfv.fit_transform(q1q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the third set of features by stacking the matrices question1_vectors, question2_vectors together\n",
    "fs3_3 = np.hstack((question1_vectors, question2_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs3_4 = ['skew_q1vec', 'skew_q2vec', 'kur_q1vec', 'kur_q2vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del([tfv_q1, tfv_q2, tfv, q1q2, question1_vectors, question2_vectors, svd_q1, svd_q2, q1_tfidf, q2_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check garbage collector\n",
    "# force the garbage collector to release unreferenced memory\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec embeddings\n",
    "We're going to use a pretrained Word2vec model trained on the Google News corpus.\n",
    "\n",
    "__Link:__ https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "\n",
    "__Package required__:\n",
    "  1. Gensim to load the Word2vec features\n",
    "  2. pyemd to relate two Word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        'data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\peiya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\peiya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from nltk.corpus import stopwords # stopwords such as a, an, the, ... will be ommited\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# convert sentences to vectors\n",
    "def sent2vec(sent, model):\n",
    "    # M is a list containing all the important words \n",
    "    M = []\n",
    "    words = word_tokenize(str(sent).lower())\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            if word.isalpha(): # check if word is alphabetic character\n",
    "                if word in model: # check if word is already part of Word2Vec\n",
    "                    M.append(model[word])\n",
    "    \n",
    "    # convert M to array for better processing\n",
    "    M = np.array(M)\n",
    "    if len(M) > 0:\n",
    "        v = M.sum(axis=0)\n",
    "        return v / np.sqrt((v**2).sum()) # standardize vector\n",
    "    else :\n",
    "        return model.get_vector('null') # return a null vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2Vec vectors for question 1 and 2\n",
    "w2v_q1 = np.array([sent2vec(q, model) for q in data.question1])\n",
    "w2v_q2 = np.array([sent2vec(q, model) for q in data.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement all the different distance measures between the vectors of the Word2vec embeddings of the Quora questions\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "\n",
    "data['cosine_distance'] = [cosine(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data['cityblock_distance'] = [cityblock(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data['jaccard_distance'] = [jaccard(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data['canberra_distance'] = [canberra(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data['euclidean_distance'] = [euclidean(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data['minkowski_distance'] = [minkowski(x,y,3) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data['braycurtis_distance'] = [braycurtis(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the feature names related to distances are gathered under the list fs4_1\n",
    "fs4_1 = ['cosine_distance', 'cityblock_distance', \n",
    "         'jaccard_distance', 'canberra_distance', \n",
    "         'euclidean_distance', 'minkowski_distance',\n",
    "         'braycurtis_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec matrices for the two questions are horizontally stacked and stored away in w2v\n",
    "w2v = np.hstack((w2v_q1, w2v_q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11460"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# release memory\n",
    "del([w2v_q1, w2v_q2])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement Word Mover's Distance returning the distance between two questions\n",
    "def wmd(s1, s2, model):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    \n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply wmd function\n",
    "data['wmd'] = data.apply(lambda x: wmd(x['question1'], x['question2'], model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace=True) # normalizing word2vec vectors \n",
    "data['norm_wmd'] = data.apply(lambda x: wmd(x['question1'], x['question2'], model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all feature names related to wmd are gathered under the list fs4_2\n",
    "fs4_2 = ['wmd', 'norm_wmd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# release memory\n",
    "del([model])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=16981487616, available=5024292864, percent=70.4, used=11957194752, free=5024292864)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the memory again\n",
    "import psutil\n",
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing the data\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input X and output y\n",
    "X = data[fs_1+fs_2+fs3_4+fs4_1+fs4_2] # filter the fs_1, fs_2, fs3_4, fs4_1, and fs4_2 set of variables\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(0).values # preprocessing inf and NaN\n",
    "X = scaler.fit_transform(X) # standardize the data\n",
    "X = np.hstack((X, fs3_3)) # stack the fs3_3 sparse SVD data matrix horizontally\n",
    "\n",
    "y = data.is_duplicate.values # get the is_duplicate label\n",
    "y = y.astype('float32').reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# separating 1/10 of the data for validation purposes\n",
    "n_all, _ = y.shape\n",
    "idx = np.arange(n_all)\n",
    "np.random.shuffle(idx)\n",
    " \n",
    "n_split = n_all // 10\n",
    "idx_val = idx[:n_split]\n",
    "idx_train = idx[n_split:]\n",
    " \n",
    "x_train = X[idx_train]\n",
    "y_train = np.ravel(y[idx_train]) # return a contiguous flattened array.\n",
    " \n",
    "x_val = X[idx_val]\n",
    "y_val = np.ravel(y[idx_val]) # return a contiguous flattened array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy: 0.744\n"
     ]
    }
   ],
   "source": [
    "# train and predict with Logistic Regression model\n",
    "logres = linear_model.LogisticRegression(C=0.1, solver='sag', max_iter=1000)\n",
    "\n",
    "logres.fit(x_train, y_train)\n",
    "lr_preds = logres.predict(x_val)\n",
    "log_res_accuracy = np.sum(lr_preds == y_val) / len(y_val)\n",
    "print(\"Logistic regression accuracy: {:.3f}\".format(log_res_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.687437\ttrain-error:0.297336\tvalid-logloss:0.687544\tvalid-error:0.297583\n",
      "Multiple eval metrics have been passed: 'valid-error' will be used for early stopping.\n",
      "\n",
      "Will train until valid-error hasn't improved in 50 rounds.\n",
      "[100]\ttrain-logloss:0.501917\ttrain-error:0.261259\tvalid-logloss:0.503814\tvalid-error:0.263252\n",
      "[200]\ttrain-logloss:0.468034\ttrain-error:0.244923\tvalid-logloss:0.470746\tvalid-error:0.246209\n",
      "[300]\ttrain-logloss:0.451674\ttrain-error:0.234383\tvalid-logloss:0.454916\tvalid-error:0.236637\n",
      "[400]\ttrain-logloss:0.441258\ttrain-error:0.227559\tvalid-logloss:0.445179\tvalid-error:0.230874\n",
      "[500]\ttrain-logloss:0.434009\ttrain-error:0.222786\tvalid-logloss:0.43852\tvalid-error:0.226793\n",
      "[600]\ttrain-logloss:0.428219\ttrain-error:0.218922\tvalid-logloss:0.43332\tvalid-error:0.223033\n",
      "[700]\ttrain-logloss:0.423202\ttrain-error:0.215582\tvalid-logloss:0.428955\tvalid-error:0.220584\n",
      "[800]\ttrain-logloss:0.418775\ttrain-error:0.212545\tvalid-logloss:0.425191\tvalid-error:0.218729\n",
      "[900]\ttrain-logloss:0.414681\ttrain-error:0.209481\tvalid-logloss:0.421651\tvalid-error:0.216107\n",
      "[1000]\ttrain-logloss:0.411204\ttrain-error:0.206774\tvalid-logloss:0.41872\tvalid-error:0.214104\n",
      "[1100]\ttrain-logloss:0.408038\ttrain-error:0.204573\tvalid-logloss:0.416151\tvalid-error:0.212471\n",
      "[1200]\ttrain-logloss:0.405246\ttrain-error:0.202888\tvalid-logloss:0.413903\tvalid-error:0.211111\n",
      "[1300]\ttrain-logloss:0.402657\ttrain-error:0.201203\tvalid-logloss:0.411891\tvalid-error:0.209849\n",
      "[1400]\ttrain-logloss:0.40002\ttrain-error:0.19945\tvalid-logloss:0.409797\tvalid-error:0.208143\n",
      "[1500]\ttrain-logloss:0.397802\ttrain-error:0.197908\tvalid-logloss:0.408049\tvalid-error:0.207302\n",
      "[1600]\ttrain-logloss:0.395644\ttrain-error:0.196454\tvalid-logloss:0.406409\tvalid-error:0.205867\n",
      "[1700]\ttrain-logloss:0.393663\ttrain-error:0.194976\tvalid-logloss:0.404911\tvalid-error:0.204853\n",
      "[1800]\ttrain-logloss:0.391504\ttrain-error:0.193483\tvalid-logloss:0.403299\tvalid-error:0.203913\n",
      "[1900]\ttrain-logloss:0.389431\ttrain-error:0.191952\tvalid-logloss:0.401764\tvalid-error:0.202849\n",
      "[2000]\ttrain-logloss:0.387563\ttrain-error:0.190639\tvalid-logloss:0.400415\tvalid-error:0.201736\n",
      "[2100]\ttrain-logloss:0.385483\ttrain-error:0.189078\tvalid-logloss:0.398881\tvalid-error:0.201019\n",
      "[2200]\ttrain-logloss:0.383437\ttrain-error:0.187684\tvalid-logloss:0.39729\tvalid-error:0.199683\n",
      "[2300]\ttrain-logloss:0.381647\ttrain-error:0.186321\tvalid-logloss:0.396031\tvalid-error:0.199115\n",
      "[2400]\ttrain-logloss:0.37981\ttrain-error:0.185095\tvalid-logloss:0.394765\tvalid-error:0.197482\n",
      "[2500]\ttrain-logloss:0.378172\ttrain-error:0.184073\tvalid-logloss:0.393613\tvalid-error:0.196592\n",
      "[2600]\ttrain-logloss:0.3766\ttrain-error:0.183034\tvalid-logloss:0.392536\tvalid-error:0.196196\n",
      "[2700]\ttrain-logloss:0.374973\ttrain-error:0.181894\tvalid-logloss:0.391404\tvalid-error:0.195577\n",
      "[2800]\ttrain-logloss:0.373318\ttrain-error:0.180583\tvalid-logloss:0.390296\tvalid-error:0.194761\n",
      "[2900]\ttrain-logloss:0.371763\ttrain-error:0.1795\tvalid-logloss:0.389291\tvalid-error:0.194464\n",
      "Stopping. Best iteration:\n",
      "[2852]\ttrain-logloss:0.372491\ttrain-error:0.179973\tvalid-logloss:0.389781\tvalid-error:0.19439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train and predict with XGBoost\n",
    "params = dict()\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = ['logloss', 'error']\n",
    "params['eta'] = 0.02 # learning rate\n",
    "params['max_depth'] = 4 # shallow depth to prevent overfit\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_val, label=y_val)\n",
    "\n",
    "# create watch list to keep an eye on the valid set early stopping\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "# train the xgb model, stop if validation doesn't decrease for over 50 steps\n",
    "bst = xgb.train(params, d_train, 5000, watchlist, early_stopping_rounds=50, verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8055356303643424\n"
     ]
    }
   ],
   "source": [
    "xgb_preds = (bst.predict(d_valid) >= 0.5).astype(int)\n",
    "xgb_accuracy = np.sum(xgb_preds==y_val) / len(y_val)\n",
    "\n",
    "print(xgb_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Quora's Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe from the Quora dataset\n",
    "try:\n",
    "    df = data[['question1', 'question2', 'is_duplicate']]\n",
    "except:\n",
    "    df = pd.read_csv('data/quora_duplicate_questions.tsv', sep='\\t')\n",
    "    df = df.drop(['id', 'qid1', 'qid2'], axis=1)\n",
    "    \n",
    "df = df.fillna('')\n",
    "y = df.is_duplicate.values\n",
    "y = y.astype('float32').reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the data and convert the data to sequences\n",
    "Tokenizer = tf.keras.preprocessing.text.Tokenizer\n",
    "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences\n",
    "\n",
    "# set the maximum number of words to 200,000\n",
    "tk = Tokenizer(num_words=200000) \n",
    "\n",
    "# set maximum sequence length to 40, \n",
    "# if sentence has more than 40 words, it will be cutoff to 40 words only\n",
    "max_len = 40 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the tk on the concatenated list of the first and second questions,\n",
    "# in order to learn all the possible word terms in the corpus\n",
    "tk.fit_on_texts(list(df.question1) + list(df.question2))\n",
    "x1 = tk.texts_to_sequences(df.question1)\n",
    "x1 = pad_sequences(x1, maxlen=max_len)\n",
    " \n",
    "x2 = tk.texts_to_sequences(df.question2)\n",
    "x2 = pad_sequences(x2, maxlen=max_len)\n",
    "\n",
    "# word_index dictionary contains all the tokenized words, \n",
    "# paird with an corresponding assigned index\n",
    "word_index = tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d8567e27f742c4a604d820cac3d5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load GloVe embeddings\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300), dtype='float32')\n",
    "\n",
    "glove_zip = zipfile.ZipFile('data/glove.840B.300d.zip')\n",
    "glove_file = glove_zip.filelist[0]\n",
    "\n",
    "f_in = glove_zip.open(glove_file)\n",
    "\n",
    "for line in tqdm(f_in):\n",
    "    values = line.split(b' ')\n",
    "    word = values[0].decode()\n",
    "    \n",
    "    if word not in word_index:\n",
    "        continue \n",
    "    i = word_index[word]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_matrix[i, :] = coefs\n",
    "    \n",
    "f_in.close()\n",
    "glove_zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prepare batches to feed into the DNN\n",
    "# the function takes the question seqs and based on step param (batch size) \n",
    "# to return a list of lists, with internal lists are the seq batches to be learned\n",
    "def prepare_batches(seq, step):\n",
    "    n = len(seq)\n",
    "    res = []\n",
    "    \n",
    "    for i in range(0, n, step):\n",
    "        res.append(seq[i:i+step])\n",
    "        \n",
    "    return res\n",
    "\n",
    "# convolutional layer\n",
    "def conv1d(inputs, num_filters, filter_size, padding='same'):\n",
    "    he_std = np.sqrt(2 / (filter_size * num_filters))\n",
    "    out = tf.layers.conv1d(\n",
    "        inputs=inputs, filters=num_filters, padding=padding,\n",
    "        kernel_size=filter_size,\n",
    "        activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=he_std))\n",
    "    return out\n",
    "\n",
    "\n",
    "# maxpooling layer\n",
    "def maxpool1d_global(X):\n",
    "    out = tf.reduce_max(X, axis=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "# dense layer\n",
    "def dense(X, size, activation=None):\n",
    "    # He initialization procedures\n",
    "    he_std = np.sqrt(2/ int(X.shape[1]))\n",
    "    \n",
    "    out = tf.layers.dense(X, units=size,\n",
    "                         activation=activation,\n",
    "                         kernel_initializer=tf.random_normal_initializer(stddev=he_std))\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "# time distributed dense layer\n",
    "def time_distributed_dense(X, dense_size):\n",
    "    shape = X.shape.as_list()\n",
    "    assert len(shape) == 3\n",
    "    _, w, d = shape\n",
    " \n",
    "    X_reshaped = tf.reshape(X, [-1, d])\n",
    "    H = dense(X_reshaped, dense_size, tf.nn.relu)\n",
    " \n",
    "    return tf.reshape(H, [-1, w, dense_size])\n",
    "\n",
    "\n",
    "# lstm layer\n",
    "def lstm(X, size_hidden, size_out):\n",
    "    with tf.variable_scope('lstm_%d' % np.random.randint(0, 100)):\n",
    "        he_std = np.sqrt(2 / (size_hidden * size_out))\n",
    "        W = tf.Variable(tf.random_normal([size_hidden, size_out], stddev=he_std))\n",
    "        b = tf.Variable(tf.zeros([size_out]))\n",
    " \n",
    "        size_time = int(X.shape[1])\n",
    "        X = tf.unstack(X, size_time, axis=1)\n",
    " \n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(size_hidden, forget_bias=1.0)\n",
    "        outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, X, dtype='float32')\n",
    "        out = tf.matmul(outputs[-1], W) + b\n",
    " \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "max_features = 200000\n",
    "filter_length = 5\n",
    "nb_filter = 64\n",
    "pool_length = 4\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-69-fdbe6fc14b44>:20: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv1d instead.\n",
      "WARNING:tensorflow:From <ipython-input-71-725566b44a15>:32: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From C:\\Users\\peiya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-71-725566b44a15>:38: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-69-fdbe6fc14b44>:64: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-69-fdbe6fc14b44>:65: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\peiya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:514: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\peiya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\peiya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "# initialize tensorflow graph\n",
    "graph = tf.Graph()\n",
    "graph.seed = 1\n",
    "\n",
    "with graph.as_default():\n",
    "    place_q1 = tf.placeholder(tf.int32, shape=(None, max_len))\n",
    "    place_q2 = tf.placeholder(tf.int32, shape=(None, max_len))\n",
    "    place_y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    place_training = tf.placeholder(tf.bool, shape=())\n",
    " \n",
    "    glove = tf.Variable(embedding_matrix, trainable=False)\n",
    "    q1_glove_lookup = tf.nn.embedding_lookup(glove, place_q1)\n",
    "    q2_glove_lookup = tf.nn.embedding_lookup(glove, place_q2)\n",
    " \n",
    "    emb_size = len(word_index) + 1\n",
    "    emb_dim = 300\n",
    "    emb_std = np.sqrt(2 / emb_dim)\n",
    "    emb = tf.Variable(tf.random_uniform([emb_size, emb_dim], -emb_std, emb_std))\n",
    "    q1_emb_lookup = tf.nn.embedding_lookup(emb, place_q1)\n",
    "    q2_emb_lookup = tf.nn.embedding_lookup(emb, place_q2)\n",
    "   \n",
    "    model1 = q1_glove_lookup\n",
    "    model1 = time_distributed_dense(model1, 300)\n",
    "    model1 = tf.reduce_sum(model1, axis=1)\n",
    " \n",
    "    model2 = q2_glove_lookup\n",
    "    model2 = time_distributed_dense(model2, 300)\n",
    "    model2 = tf.reduce_sum(model2, axis=1)\n",
    " \n",
    "    model3 = q1_glove_lookup\n",
    "    model3 = conv1d(model3, nb_filter, filter_length, padding='valid')\n",
    "    model3 = tf.layers.dropout(model3, rate=0.2, training=place_training)\n",
    "    model3 = conv1d(model3, nb_filter, filter_length, padding='valid')\n",
    "    model3 = maxpool1d_global(model3)\n",
    "    model3 = tf.layers.dropout(model3, rate=0.2, training=place_training)\n",
    "    model3 = dense(model3, 300)\n",
    "    model3 = tf.layers.dropout(model3, rate=0.2, training=place_training)\n",
    "    model3 = tf.layers.batch_normalization(model3, training=place_training)\n",
    " \n",
    "    model4 = q2_glove_lookup\n",
    "    model4 = conv1d(model4, nb_filter, filter_length, padding='valid')\n",
    "    model4 = tf.layers.dropout(model4, rate=0.2, training=place_training)\n",
    "    model4 = conv1d(model4, nb_filter, filter_length, padding='valid')\n",
    "    model4 = maxpool1d_global(model4)\n",
    "    model4 = tf.layers.dropout(model4, rate=0.2, training=place_training)\n",
    "    model4 = dense(model4, 300)\n",
    "    model4 = tf.layers.dropout(model4, rate=0.2, training=place_training)\n",
    "    model4 = tf.layers.batch_normalization(model4, training=place_training)\n",
    " \n",
    "    model5 = q1_emb_lookup\n",
    "    model5 = tf.layers.dropout(model5, rate=0.2, training=place_training)\n",
    "    model5 = lstm(model5, size_hidden=300, size_out=300)\n",
    " \n",
    "    model6 = q2_emb_lookup\n",
    "    model6 = tf.layers.dropout(model6, rate=0.2, training=place_training)\n",
    "    model6 = lstm(model6, size_hidden=300, size_out=300)\n",
    " \n",
    "    merged = tf.concat([model1, model2, model3, model4, model5, model6], axis=1)\n",
    "    #merged = tf.concat([model1, model2], axis=1)\n",
    "    merged = tf.layers.batch_normalization(merged, training=place_training)\n",
    " \n",
    "    for i in range(5):\n",
    "        merged = dense(merged, 300, activation=tf.nn.relu)\n",
    "        merged = tf.layers.dropout(merged, rate=0.2, training=place_training)\n",
    "        merged = tf.layers.batch_normalization(merged, training=place_training)\n",
    " \n",
    "    merged = dense(merged, 1, activation=tf.nn.sigmoid)\n",
    "   \n",
    "    loss = tf.losses.log_loss(place_y, merged)\n",
    " \n",
    "    prediction = tf.round(merged)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(place_y, prediction), 'float32'))\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    " \n",
    "    # for batchnorm\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        step = opt.minimize(loss)\n",
    " \n",
    "    init = tf.global_variables_initializer()\n",
    " \n",
    "session = tf.Session(config=None, graph=graph)\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "# split the dataset into a training part (9/10) and a testing one (1/10).\n",
    "n_all, _ = y.shape\n",
    "idx = np.arange(n_all)\n",
    "np.random.shuffle(idx)\n",
    " \n",
    "n_split = n_all // 10\n",
    "idx_val = idx[:n_split]\n",
    "idx_train = idx[n_split:]\n",
    " \n",
    "x1_train = x1[idx_train]\n",
    "x2_train = x2[idx_train]\n",
    "y_train = y[idx_train]\n",
    " \n",
    "x1_val = x1[idx_val]\n",
    "x2_val = x2[idx_val]\n",
    "y_val = y[idx_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23eebc9781b24b978267152bf05b0a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=948), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, accuracy: 0.797\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7ab1691163431a923cbb9dfcda4fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=948), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, accuracy: 0.803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e076faa5c94625855f195cd31af4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=948), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, accuracy: 0.810\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d4a27fb4d14e03a09223334cc43115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=948), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, accuracy: 0.810\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00ed252927a4593b79739fc3b30591c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=948), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, accuracy: 0.808\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f43bc4364449b3836c5c695edfda2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=948), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, accuracy: 0.810\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc798c3140a4aed970cf76f1a533e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=948), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, accuracy: 0.817\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e592f698e3f646908ba61adcc133200d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=948), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, accuracy: 0.816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4aca7a448f491f808391702de65d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=948), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, accuracy: 0.816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c4436755b9459ea6085a783a9af21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=948), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, accuracy: 0.816\n"
     ]
    }
   ],
   "source": [
    "val_idx = np.arange(y_val.shape[0])\n",
    "val_batches = prepare_batches(val_idx, 5000)\n",
    "\n",
    "no_epochs = 10\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "for i in range(no_epochs):\n",
    "    np.random.seed(i)\n",
    "    \n",
    "    train_idx_shuffle = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(train_idx_shuffle)\n",
    "    batches = prepare_batches(train_idx_shuffle, 384)\n",
    "   \n",
    "    progress = tqdm(total=len(batches))\n",
    "    for idx in batches:\n",
    "        feed_dict = {\n",
    "            place_q1: x1_train[idx],\n",
    "            place_q2: x2_train[idx],\n",
    "            place_y: y_train[idx],\n",
    "            place_training: True,\n",
    "        }\n",
    "        _, acc, l = session.run([step, accuracy, loss], feed_dict)\n",
    "        progress.update(1)\n",
    "        progress.set_description(('{:.3f} / {:.3f}').format(acc, l))\n",
    " \n",
    " \n",
    "    y_pred = np.zeros_like(y_val)\n",
    "    for idx in val_batches:\n",
    "        feed_dict = {\n",
    "            place_q1: x1_val[idx],\n",
    "            place_q2: x2_val[idx],\n",
    "            place_y: y_val[idx],\n",
    "            place_training: False,\n",
    "        }\n",
    "        y_pred[idx, :] = session.run(prediction, feed_dict)\n",
    " \n",
    "    print('Epoch {}, accuracy: {:.3f}'.format(i, np.mean(y_val == y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to numpy.ndarray.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-0db0e5a4c8de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Why do people ask similar questions on Quora multiple times?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer: {0:.2f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to numpy.ndarray.__format__"
     ]
    }
   ],
   "source": [
    "# evaluate model on a sample\n",
    "def convert_text(txt, tokenizer, padder):\n",
    "    x = tokenizer.texts_to_sequences(txt)\n",
    "    x = padder(x, maxlen=max_len)\n",
    "    return x  \n",
    "\n",
    "def evaluate_questions(a, b, tokenizer, padder, pred):\n",
    "    feed_dict = {\n",
    "            place_q1: convert_text([a], tk, pad_sequences),\n",
    "            place_q2: convert_text([b], tk, pad_sequences),\n",
    "            place_y: np.zeros((1,1)),\n",
    "            place_training: False,\n",
    "        }\n",
    "    return session.run(pred, feed_dict)\n",
    "    \n",
    "isduplicated = lambda a, b: evaluate_questions(a, b, tk, pad_sequences, prediction)\n",
    "\n",
    "a = \"Why are there so many duplicated questions on Quora?\"\n",
    "b = \"Why do people ask similar questions on Quora multiple times?\"\n",
    "\n",
    "print(\"Answer: %0.2f\" % isduplicated(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
